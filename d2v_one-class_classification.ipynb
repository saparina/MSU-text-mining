{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Исследование векторных моделей представления текстовых данных на основе набора алгоритмов *word2vec*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи:\n",
    "- Провести экспериментальное исследование векторных моделей представления текстовых данных на основе набора алгоритмов *word2vec* в задаче одноклассовой классификации текстовых документов;\n",
    "- Сравнить качество классификации текстовых документов с использованием исследуемых моделей и модели «мешок слов»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применение *doc2vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from random import shuffle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import OneClassSVM\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём итератор, который необходим для дальнейшего использования модели *doc2vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DocsIterator(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            #for lines in doc:\n",
    "            doc_words = doc.split()\n",
    "            #shuffle(doc_words)\n",
    "            yield LabeledSentence(words=doc_words , tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Набор данных\n",
    "Используем набор данных Enron. Это тексты из переписки 15 сотрудников компании Enron за 2000 и 2001 годы,всего 11941 текстовый документ, которые были разбиты на 118 экспериментальных диапазонов. Использовались оригинальный набор и обработанный с помощью алгоритма, разработанного в лаборатории ТП АСВК."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считывание данных. Создаём текстовый корпус и метки для задачи одноклассовой классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(user_id, element_list, path, data, labels = None):\n",
    "    for docs in element_list:\n",
    "        with open(path + '/' + str(docs) + '.term', 'r') as file:\n",
    "            data.append(file.read())\n",
    "            if labels != None:\n",
    "                labels.append(user_id + docs)\n",
    "        file.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fill_data_labels(data_list, dataset_path, inf_shrink):\n",
    "    data = [] \n",
    "    labels = []  \n",
    "    for (user_name, user_id, element_list) in data_list:\n",
    "        path = os.path.join(dataset_path, user_name, inf_shrink)\n",
    "        read_data(user_id, element_list, path, data, labels)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём и обучаем модель *doc2vec*\n",
    "\n",
    "Пробуем использовать [control the learning rate](https://rare-technologies.com/doc2vec-tutorial/#training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(train_list, dataset_path, inf_shrink, dm, mode, size, window):\n",
    "    data, labels =  fill_data_labels(train_list, dataset_path, inf_shrink)\n",
    "    it = DocsIterator(data, labels)\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    if dm == 1:\n",
    "        if mode is 'dm_mean':\n",
    "            #alpha=0.025, min_alpha=0.025,\n",
    "            model = doc2vec.Doc2Vec(alpha=0.025, min_alpha=0.025, dm = dm, dm_mean = 1, \\\n",
    "                                    size = size, window = window, min_count = 2, workers=cores) \n",
    "        elif mode is 'dm_concat':\n",
    "            model = doc2vec.Doc2Vec(alpha=0.025, min_alpha=0.025, dm = dm, dm_concat = 1, \\\n",
    "                                    size = size, window = window, min_count = 2, workers=cores) \n",
    "        else:\n",
    "            model = doc2vec.Doc2Vec(alpha=0.025, min_alpha=0.025, dm = dm, size = size, \\\n",
    "                                    window = window, min_count = 2, workers=cores) \n",
    "    else:\n",
    "         model = doc2vec.Doc2Vec(alpha=0.025, min_alpha=0.025, dm = dm, size = size, \\\n",
    "                                 min_count = 2, workers=cores)      \n",
    "    model.build_vocab(it)\n",
    "    for epoch in range(10):\n",
    "        model.train(it)\n",
    "        model.alpha -= 0.002 # decrease the learning rate\n",
    "        model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Решаем задачу одноклассовой классификации для разных экспериментальных диапозонов.\n",
    "`clf_mode`: пробуем классификаторы **k-Nearest-Neighbor**  и **One-Class Support Vector Machine**. \n",
    "\n",
    "`dm`, `size`, `window`: пробуем разные варианты моделей *doc2vec*: Distributed Memory version of Paragraph Vectors (PV-DM) и Distributed Bag of Words version of Paragraph Vector (PV-DBOW), разные размеры окна и размерности пространства.\n",
    "\n",
    "Метрика качества **ROC AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process(dataset_path, clf_mode = 'kNN', dm = 1, mode = '', size = 10, window = 10):\n",
    "    auc_list1 = []\n",
    "    auc_list2 = []\n",
    "    auc_list3 = []\n",
    "    count = 0\n",
    "    for target_user_name in os.listdir(dataset_path):\n",
    "        user_path = os.path.join(dataset_path, target_user_name)\n",
    "        partition_path = os.path.join(user_path, 'partition.pickle')\n",
    "\n",
    "        partition = None\n",
    "        with open(partition_path, 'rb') as f:\n",
    "            partition = pickle.load(f)\n",
    "\n",
    "        if partition is None or not partition:\n",
    "            continue\n",
    "\n",
    "        inf_shrink = 'nmf_10'\n",
    "        inf_shrink_path = os.path.join(user_path, inf_shrink)\n",
    "\n",
    "        if not os.path.isdir(inf_shrink_path):  # Skip .pickle\n",
    "            continue\n",
    "\n",
    "        for (ds_time_series_train_list, ds_time_series_test_list) in partition:\n",
    "            #vectors\n",
    "            X_train = []\n",
    "            X_test = []\n",
    "            #labels\n",
    "            y_train = []\n",
    "            y_test= []\n",
    "            count += 1\n",
    "            model = create_model(ds_time_series_train_list, dataset_path, inf_shrink, dm, mode, size, window)\n",
    "\n",
    "            for (user_name, user_id, element_list) in ds_time_series_train_list:\n",
    "                # user_name\n",
    "                # user_id\n",
    "                # element_list - list of file id for user\n",
    "                \n",
    "                train_path = os.path.join(dataset_path, user_name, inf_shrink)\n",
    "                for docs in element_list:\n",
    "                    label = user_id + docs\n",
    "                    X_train.append(model.docvecs[label])\n",
    "                    if user_name == target_user_name:\n",
    "                        y_train.append(1)\n",
    "                    else:\n",
    "                        y_train.append(-1)                    \n",
    "\n",
    "            for (user_name, user_id, element_list) in ds_time_series_test_list:\n",
    "                # Test docs\n",
    "                if(len(element_list) == 0):\n",
    "                    continue\n",
    "                test_path = os.path.join(dataset_path, user_name, inf_shrink)\n",
    "                test_data = []\n",
    "                read_data(user_id, element_list, test_path, test_data)\n",
    "                for doc, elems in zip(test_data, element_list):\n",
    "                    words = doc.split()\n",
    "                    #shuffle(words)\n",
    "                    X_test.append(model.infer_vector(words))\n",
    "                    if user_name == target_user_name:\n",
    "                        y_test.append(1)\n",
    "                    else:\n",
    "                        y_test.append(-1) \n",
    "                            \n",
    "            if clf_mode == 'kNN':           \n",
    "                auc_score = []\n",
    "                for n_neighbors in range(1, 4):              \n",
    "                    clf = NearestNeighbors(n_neighbors = n_neighbors, algorithm='auto')\n",
    "                    clf.fit(X_train)\n",
    "                    (distances, indices) = clf.kneighbors(X_test)\n",
    "                    test_score = []\n",
    "                    for test_i in distances:\n",
    "                        test_score.append(max(test_i))\n",
    "                    roc_auc = roc_auc_score(y_test, test_score)\n",
    "                    auc_score.append(1 - roc_auc)\n",
    "                auc_list1.append(auc_score[0])\n",
    "                auc_list2.append(auc_score[1])\n",
    "                auc_list3.append(auc_score[2])\n",
    "            else:\n",
    "                clf = OneClassSVM(kernel='rbf')\n",
    "                clf.fit(X_train)\n",
    "                test_score = clf.decision_function(X_test)\n",
    "                fpr, tpr, thresholds = roc_curve(y_test, test_score, pos_label = 1)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                roc_auc = roc_auc_score(y_test, test_score)\n",
    "                auc_list1.append(roc_auc)\n",
    "\n",
    "    if clf_mode == 'kNN':\n",
    "        return np.median(auc_list1), np.median(auc_list2), np.median(auc_list3)\n",
    "    \n",
    "    return np.median(auc_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример запуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.921221231817\n"
     ]
    }
   ],
   "source": [
    "res = process('./student/', clf_mode = 'SVM', dm = 0, mode = '', size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение с bag-of-words\n",
    "Для сравнения реализуем подход bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "    others_data = []\n",
    "    for text in data:\n",
    "        words = text.split()\n",
    "        vocab = []\n",
    "        for word in words:\n",
    "            if words.count(word) < 2:\n",
    "                text = text.replace(word, '_others')\n",
    "            else:\n",
    "                vocab.append(word)\n",
    "        others_data.append(text)  \n",
    "    return others_data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_others(data, vocab):\n",
    "    others_data = []\n",
    "    for text in data:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if words.count(word) < 2 or vocab.count(word) < 1:\n",
    "                text = text.replace(word, '_others')\n",
    "        others_data.append(text)  \n",
    "    return others_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bow_process(dataset_path, clf_mode = 'kNN'):\n",
    "    auc_list1 = []\n",
    "    auc_list2 = []\n",
    "    auc_list3 = []\n",
    "    count = 0\n",
    "    for target_user_name in os.listdir(dataset_path):\n",
    "        user_path = os.path.join(dataset_path, target_user_name)\n",
    "        partition_path = os.path.join(user_path, 'partition.pickle')\n",
    "\n",
    "        partition = None\n",
    "        with open(partition_path, 'rb') as f:\n",
    "            partition = pickle.load(f)\n",
    "\n",
    "        if partition is None or not partition:\n",
    "            continue\n",
    "\n",
    "        inf_shrink = 'nmf_10'\n",
    "        inf_shrink_path = os.path.join(user_path, inf_shrink)\n",
    "\n",
    "        if not os.path.isdir(inf_shrink_path):  # Skip .pickle\n",
    "            continue\n",
    "\n",
    "        for (ds_time_series_train_list, ds_time_series_test_list) in partition:\n",
    "            #labels\n",
    "            y_train = []\n",
    "            y_test= []\n",
    "            count += 1\n",
    "    \n",
    "            vectorizer = TfidfVectorizer(use_idf = False, lowercase =  False)\n",
    "            for (user_name, user_id, element_list) in ds_time_series_train_list:\n",
    "                # user_name\n",
    "                # user_id\n",
    "                # element_list - list of file id for user\n",
    "                train_path = os.path.join(dataset_path, user_name, inf_shrink)\n",
    "                data = []\n",
    "                read_data(user_id, element_list, train_path, data)\n",
    "                train_data, vocab = build_vocab(data)\n",
    "                X_train = vectorizer.fit_transform(train_data).toarray()\n",
    "                for docs in element_list:\n",
    "                    if user_name == target_user_name:\n",
    "                        y_train.append(1)\n",
    "                    else:\n",
    "                        y_train.append(-1)   \n",
    "            \n",
    "            fl = True\n",
    "            for (user_name, user_id, element_list) in ds_time_series_test_list:\n",
    "                # Test docs\n",
    "                if(len(element_list) == 0):\n",
    "                    continue\n",
    "                test_path = os.path.join(dataset_path, user_name, inf_shrink)\n",
    "                data2 = []\n",
    "                read_data(user_id, element_list, test_path, data2)\n",
    "                test_data = bow_others(data2, vocab)\n",
    "                if fl:\n",
    "                    X_test = vectorizer.transform(test_data).toarray()\n",
    "                    fl = False\n",
    "                else:\n",
    "                    X_test = np.concatenate((X_test, vectorizer.transform(test_data).toarray()), axis = 0)\n",
    "                for doc, elems in zip(test_data, element_list):\n",
    "                    if user_name == target_user_name:\n",
    "                        y_test.append(1)\n",
    "                    else:\n",
    "                        y_test.append(-1)\n",
    "         \n",
    "            if clf_mode == 'kNN':           \n",
    "                auc_score = []\n",
    "                for n_neighbors in range(1, 4):              \n",
    "                    clf = NearestNeighbors(n_neighbors = n_neighbors, algorithm='auto')\n",
    "                    clf.fit(X_train)\n",
    "                    (distances, indices) = clf.kneighbors(X_test)\n",
    "                    test_score = []\n",
    "                    for test_i in distances:\n",
    "                        test_score.append(max(test_i))\n",
    "                    fpr, tpr, thresholds = roc_curve(y_test, test_score, pos_label = 1)\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    roc_auc = roc_auc_score(y_test, test_score)\n",
    "                    auc_score.append(1 - roc_auc)\n",
    "                auc_list1.append(auc_score[0])\n",
    "                auc_list2.append(auc_score[1])\n",
    "                auc_list3.append(auc_score[2])  \n",
    "                print(auc_score[0], auc_score[1], auc_score[2])\n",
    "            else:\n",
    "                clf = OneClassSVM(kernel = 'linear')\n",
    "                clf.fit(X_train)\n",
    "                test_score = clf.decision_function(X_test)\n",
    "                roc_auc = roc_auc_score(y_test, test_score)\n",
    "                auc_list1.append(roc_auc)\n",
    "    \n",
    "    if clf_mode == 'kNN':\n",
    "        return np.median(auc_list1), np.median(auc_list2), np.median(auc_list3)\n",
    "    \n",
    "    return np.median(auc_list1), np.subtract(*np.percentile(auc_list1, [75, 25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "Пример запуска (медиана результатов и iqr):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.704344270144 0.153195270473\n"
     ]
    }
   ],
   "source": [
    "res = bow_process('./student/', clf_mode = 'SVM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
